#!/usr/bin/env python2.7

import logging

##############################################################################
# Global constants

# Tags we process
KNOWN_TAGS = [
  'artist',   # (album) artist
  'year',     # album year
  'album',    # album title
  'track',    # track for metadata, e.g. 04/09
  'trackno',  # just the track number, e.g. 04
  'title',    # track title
  'disc',     # disc number
]

##############################################################################
# Helper functions
def file_hash(fname, block_size = 2**20):
  """Given the file name, returns a SHA-256 hash over the file contents"""
  with open(fname, 'rb') as f:
    import hashlib
    sha256 = hashlib.sha256()
    while True:
      data = f.read(block_size)
      if not data:
        break
      sha256.update(data)
    return sha256.hexdigest()


def read_hashes(path):
  """Reads file hashes from a .hashes file in the given directory and returns
     them."""
  import os.path
  hashname = os.path.join(path, '.hashes')
  hashes = {}
  import csv
  converted = False
  try:
    with open(hashname, 'rt') as hashfile:
      reader = csv.reader(hashfile)
      for row in reader:
        if len(row) > 2:
          hashes[row[0]] = (row[1], row[2])
        else:
          if row[1].startswith('('):
            # Workaround for messed up hash files
            tmp = row[1].strip('()').split(',')
            tmp = [t.strip('\'" ') for t in tmp]
            hashes[row[0]] = tuple(tmp)
            converted = True
          else:
            hashes[row[0]] = (row[1], None)
  except IOError as e:
    pass # no hash file/no hashes

  return hashes, converted


def write_hashes(path, hashes):
  """Writes file hashes to a .hashes file in the given directory."""
  import os.path
  hashname = os.path.join(path, '.hashes')
  import csv
  try:
    with open(hashname, 'wt') as hashfile:
      writer = csv.writer(hashfile)
      for row in hashes.items():
        towrite = [row[0]] + list(row[1])
        writer.writerow(towrite)
      hashfile.close()
  except IOError as e:
    return False
  return True


def normalize_meta(meta):
  """Given metadata from e.g. get_meta(), normalizes it - that is, it tries to
     detect which non-ID3v2.3 items are given and to convert them to ID3v2.3
     items if possible."""

  if 'track' in meta:
    track = meta['track']
    total = -1

    if track.find('/') < 0:
      # No total in track field
      track = int(track)
      try:
        total = int(meta['TOTALTRACKS'])
      except KeyError:
        try:
          total = int(meta['TRACKTOTAL'])
        except KeyError:
          pass
    else:
      # total in track field
      s = track.split('/')
      track = int(s[0])
      total = int(s[1])

    meta['trackno'] = '%02d' % track

    if total > 0:
      meta['track'] = '%s/%02d' % (meta['trackno'], total)
    else:
      meta['track'] = meta['trackno']

  if 'disc' not in meta:
    meta['disc'] = '1' # most often the case

  return meta


def match_meta_pattern(meta, fname, pattern):
  """Tries to parse the given file name according to the given pattern, and
     provides the results in the meta dict."""
  # First split the pattern into the number of components we need to recognize
  # independently.
  import re
  components = pattern.split('/')

  # Process components back to front; that way, if a component doesn't match,
  # we can still provide partial results
  remainder = fname[:]
  while len(components):
    comp = components.pop()

    # Grab the last part of the filename, that's what we check this
    # component against.
    import os.path
    remainder, tail = os.path.split(remainder)

    # Try to match the current component against the file name's last part.
    import re
    res = re.search(comp, tail)
    if not res:
      break # no match, stop processing
    res = res.groupdict()

    # Add results to meta
    for key in KNOWN_TAGS:
      if key in res and (key not in meta or not meta[key]):
        meta[key] = res[key]

  return meta


def get_meta(fname, pattern = None):
  """Extract audio metadata from the named file."""
  import subprocess, tempfile

  # Metadata from file name
  meta = {}
  if pattern:
    meta = match_meta_pattern(meta, fname, pattern)

  # Try to extract metadata. It may fail
  with tempfile.NamedTemporaryFile(mode = 'rt') as metafile:
    cmd = [
      'ffmpeg',
      '-i',
      fname,
      '-y',
      '-f',
      'ffmetadata',
      metafile.name,
    ]
    try:
      subprocess.check_output(cmd, stderr = subprocess.STDOUT)
    except subprocess.CalledProcessError as ex:
      return meta

    for row in metafile.readlines():
      row = row.split('=')
      if len(row) < 2:
        continue
      value = row[1].strip('\n')
      if value:
        meta[row[0]] = value

  return meta


def process_file(source_full, options):
  """Actual work performed on each item in the worker queue happens here."""
  logging.debug('Start processing "%s"...' % source_full)

  import os.path
  source_base, source_name = os.path.split(source_full)
  basename, ext = os.path.splitext(source_name)


  # First we gather some metadata on the source file. This is necessary amongst
  # other things to ensure we find a good target path.
  meta = normalize_meta(get_meta(source_full, options['metadata']))
  logging.debug('Meta of "%s" is: %s' % (source_full, meta))

  # Next, we can establish the last components of the target file name.
  target_pattern = options['target_pattern'] % meta
  target_base, target_name = os.path.split(target_pattern)
  target_base = os.path.join(options['destination'], target_base)
  target_full = os.path.join(target_base, target_name)

  # Try to read hashes from the target_base
  hashes, converted = read_hashes(target_base)
  if converted:
    write_hashes(target_base, hashes)

  # Generate a hash of the given file. We need this to check against the
  # hashes we may just have read
  newhash = file_hash(source_full)
  logging.debug('Hash of "%s" is: %s' % (source_full, newhash))

  # If the target doesn't exist, we obviously need to process, hashes or no
  # hashes.
  if os.path.exists(target_full):
    # Check for the name, in case it's an old hash format
    if source_name in hashes:
      if hashes[source_name][0] == newhash:
        # File was processed, but update to new style hash
        logging.debug('Old hash for "%s" detected, updating.' % source_full)
        del(hashes[source_name])
        hashes[basename] = (newhash, source_full)
        write_hashes(target_base, hashes)

    # New hashes
    if basename in hashes:
      if hashes[basename][0] == newhash:
        # Already processed, skipping
        logging.debug('File "%s" is unchanged.' % source_full)
        return
      elif hashes[basename][1] == source_name or hashes[basename][1] == source_full:
        # Source updated
        logging.debug('Updated file "%s" needs to be processed!' % source_full)
      else:
        # Exists from other source, skipping
        logging.debug('Target exists from other source "%s", skipping' % hashes[basename][1])
        return
    else:
      # New file, processing
      logging.debug('New file "%s" needs to be processed!' % source_full)
  else:
    # Target didn't exist.
    logging.debug('New file "%s" needs to be processed!' % source_full)

  # Create target directory
  import os
  try:
    os.makedirs(target_base)
  except OSError as e:
    pass # path exists

  # Convert!
  cmd = [
    'ffmpeg',
    '-y',
    '-i',
    source_full,
    '-acodec',
    'libmp3lame',
    '-aq',
    '0',
    '-map_metadata',
    '0',
    '-id3v2_version',
    '3',
  ]
  for key in KNOWN_TAGS:
    if key in meta:
      cmd += [ '-metadata', '%s=%s' % (key, meta[key]) ]
  cmd += [ target_full ]

  import subprocess
  out = None
  try:
    logging.info('Processing "%s" to "%s"...' % (source_full, target_full))
    out = subprocess.check_output(cmd, stderr = subprocess.STDOUT)
    hashes[basename] = (newhash, source_full)
    write_hashes(target_base, hashes)
    logging.info('Done with "%s".' % source_full)
  except subprocess.CalledProcessError as ex:
    logging.exception('Error processing "%s"!' % source_full)



class ExitEvent(object):
  pass

class FileEvent(object):
  def __init__(self, fname):
    self.fname = fname


class Worker(object):
  """Loop over the queue and try to process each file in it."""
  def __init__(self, options, queue, condition):
    self.options = options
    self.queue = queue
    self.condition = condition

  def signal_end(self):
    self.condition.acquire()
    self.condition.notify()
    self.condition.release()

  def __call__(self):
    try:
      while True:
        event = self.queue.get()

        if isinstance(event, ExitEvent):
          self.queue.task_done()
          self.signal_end()
          return

        # Always FileEvent
        process_file(event.fname, self.options)
        self.queue.task_done()
    except Exception as err:
      logging.exception('Worker died abnormally!')

      self.signal_end()
      return


##############################################################################
# File processing
def monitor(queue, options):
  """Watch all source directories for changes, adding each file that matches
     to the queue."""
  from watchdog.observers import Observer
  from watchdog.events import FileSystemEventHandler

  class MonitorHandler(FileSystemEventHandler):
    def __init__(self, **kw):
      super(MonitorHandler, self).__init__()

    def on_modified(self, event):
      self.try_schedule(event.src_path)

    def on_moved(self, event):
      # TODO We can be more clever about it, by moving the already processed file.
      # but whatever, the point isn't to mirror the source(s) exactly, it's to
      # provide mp3 versions of them.
      self.try_schedule(event.dest_path)

    def try_schedule(self, path):
      import re
      if re.match(options['exclude'], path):
        return
      if not re.match(options['include'], path):
        return
      queue.put(FileEvent(path))


  handler = MonitorHandler(regexes = options['include'],
      ignore_regexes = options['exclude'])

  observer = Observer()

  for source in options['sources']:
    import os.path
    if os.path.isdir(source):
      observer.schedule(handler, source, recursive = True)
    else:
      logging.warning('Skipping "%s", not a directory.\n' % source)

  observer.start()


def run_once(queue, options):
  """Run through the source directories once, adding each file that matches
     to the queue."""
  for source in options['sources']:
    import os, os.path
    for root, dirs, files in os.walk(source):
      # Filter dirs, files, so we don't even bother to process further
      import re
      dirs[:] = [d for d in dirs if not re.match(options['exclude'], d, re.IGNORECASE)]

      files[:] = [f for f in files if not re.match(options['exclude'], f, re.IGNORECASE)]
      files[:] = [f for f in files if re.match(options['include'], f, re.IGNORECASE)]

      for fname in files:
        path = os.path.join(root, fname)
        queue.put(FileEvent(path))

  # proces some stuff
  queue.put(ExitEvent())


def process(options):
  """Main processing loop"""

  # The basic concept is simple: we set up a worker thread trying to process
  # entries in a queue. The queue contains two types of entries, either source
  # file entries, or a special "exit" entry.
  # Then the different run modes (run once vs. monitoring) come down to how
  # the queue is filled.
  # - In 'run once' mode, the queue is filled via an os.walk() loop. The
  #   script exits once it sees the special exit event sent when os.walk()
  #   finishes.
  # - In monitoring mode, the queue is filled from watchdog events, and no
  #   exit entry is ever put into the queue.

  # With the above said, let's create the queue
  from Queue import Queue
  queue = Queue()

  # Start the worker thread
  from threading import Thread, Condition
  exit_cond = Condition()
  worker = Thread(target = Worker(options, queue, exit_cond), name = 'worker')
  worker.daemon = True
  worker.start()

  # Now fill the queue depending on run mode
  if options['watch']:
    monitor(queue, options)
  else:
    run_once(queue, options)

  # Wait for worker to exit
  try:
    while True:
      exit_cond.acquire()
      exit_cond.wait(1)
      exit_cond.release()

      if not worker.is_alive():
        break
  except KeyboardInterrupt:
    pass

  logging.info('Exit.')


##############################################################################
# Main entry point
def main():
  DEFAULTS = {
    'include': ['*.flac', '*.wav'],
    'exclude': ['@eaDir'],
    'metadata': r'(?P<artist>.+)/(?P<year>[^- ]+) *- *(?P<album>.+)/(?P<track>[^- ]+) *- *(?P<title>.+)\..*',
  }

  import argparse
  parser = argparse.ArgumentParser(
      description = 'Transcode high quality audio files automatically to mp3',
      epilog = """Include and exclude patterns are shell style patterns. See
the shell documentation or the Python `fnmatch` module documentation for
details.

Processing is limited to files that are in the source directories, are matched
by one of the include patterns, and are not matched by one of the exclude
patterns.

Include patterns are applied to the full file path name, but exclude patterns
are also applied to full directory path names, to more easily exclude an entire
sub directory tree.
"""
  )

  parser.add_argument('destination', metavar = 'DST', type = str, nargs = '?',
      help = 'Directory in which to store processed files.')
  parser.add_argument('sources', metavar = 'SRC', type = str, nargs = '*',
      help = 'Directory with source files.')

  parser.add_argument('-c', '--config', metavar = 'FILE', type = str,
      default = '/etc/transcode.cfg',
      help = 'Location of the configuration file. Defaults to `/etc/transcode.cfg`.')

  parser.add_argument('-t', '--target-pattern', metavar = 'PATTERN', type = str,
      default = '@(artist)/@(year) - @(album)/@(trackno) - @(title).mp3',
      help = 'Pattern for how files should be structured in the target directory.')

  parser.add_argument('-i', '--include', metavar = 'PATTERN', type = str,
      action = 'append',
      help = 'Include patterns for source filenames to process. Defaults to `*.flac` and `*.wav`.')
  parser.add_argument('-e', '--exclude', metavar = 'PATTERN', type = str,
      action = 'append',
      help = 'Exclude patterns for source filenames or directories. Defaults to `@eaDir` for directories.')

  parser.add_argument('-w', '--watch', action = 'store_const', const = True,
      default = False,
      help = 'Instead of processing the source directories once and exiting, keep running and watch them for file additions and changes to process.')

  parser.add_argument('--log-file', action = 'store', metavar = 'FILE', type = str,
      default = None,
      help = 'Optional log file location. If not specified, log output goes to the terminal.')
  parser.add_argument('--log-level', action = 'store', metavar = 'LEVEL', type = str,
      default = 'INFO', choices = ['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG', ],
      help = 'Log level, one of CRITICAL, ERROR, WARNING, INFO, DEBUG.')

  parser.add_argument('--version', action = 'version', version = '%(prog)s 0.2')

  args = vars(parser.parse_args())
  for k, v in args.items():
    if not isinstance(v, bool) and not v:
      del(args[k])

  # Try processing config file, if it exists.
  import ConfigParser
  parser = ConfigParser.SafeConfigParser()
  try:
    parser.read(args['config'])
  except ConfigParser.ParsingError as err:
    logging.exception('Could not parse config')
    import sys
    sys.exit(1)

  mapping = {
    'destination': ('filesystem', 'destination', False),
    'sources': ('filesystem', 'sources'),
    'target_pattern': ('filesystem', 'target_pattern', False),
    'include': ('filters', 'include'),
    'exclude': ('filters', 'exclude'),
    'metadata': ('filters', 'metadata', False),
    'log_file': ('logging', 'file', False),
    'log_level': ('logging', 'level', False),
  }
  options = DEFAULTS.copy()
  for key, source in mapping.items():
    if parser.has_section(source[0]) and parser.has_option(source[0], source[1]):
      value = parser.get(source[0], source[1])
      if len(source) > 2 and not source[2]:
        value = value.strip()
        if value.startswith('"') and value.endswith('"'):
          value = value.strip('"')
        options[key] = value
      else:
        import re
        values = re.findall(r'(?:"[^"]*"|[^\s"])+', value)
        def unquote(item):
          if item.startswith('"') and item.endswith('"'):
            return item.strip('"')
          return item
        values = map(unquote, values)
        options[key] = values

  # Now merge command line arguments, they override the config file
  # options
  options.update(args);

  # Make the various patterns useful
  import re, fnmatch
  options['include'] = r'|'.join([fnmatch.translate(x) for x in options['include']])
  options['exclude'] = r'|'.join([fnmatch.translate(x) for x in options['exclude']]) or r'$.'
  options['target_pattern'] = options['target_pattern'].replace('@(', '%(')
  options['target_pattern'] = options['target_pattern'].replace(')', ')s')

  # Set up logging
  logopts = {}
  if 'log_file' in options:
    logopts['filename'] = options['log_file']
  logopts['level'] = getattr(logging, options['log_level'].upper());
  logopts['format'] = '[%(asctime)s] [%(levelname)s] [%(threadName)s] %(message)s'
  logging.basicConfig(**logopts)
  logging.debug('OPTIONS: %s' % options)

  # Let's do this thing!
  process(options)


if '__main__' == __name__:
  main()
